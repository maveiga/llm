"""
Script de teste para criar um dataset de exemplo e testar o RAGAS
Execute este script apÃ³s instalar as dependÃªncias para testar o sistema
"""

import asyncio
import json
from app.services.rag_service import RAGService
from app.services.ragas_service import ragas_service
from app.services.database_service import database_service

# Dataset de perguntas de teste
TEST_QUESTIONS = [
    "Qual Ã© a polÃ­tica de crÃ©dito da empresa?",
    "Como funciona o processo de verificaÃ§Ã£o cadastral?",
    "Quais documentos sÃ£o necessÃ¡rios para solicitar um emprÃ©stimo?",
    "Qual Ã© o limite de crÃ©dito baseado na renda?",
    "Como Ã© feita a anÃ¡lise de risco?",
    "Quais sÃ£o os critÃ©rios para aprovaÃ§Ã£o de crÃ©dito?",
    "Como funciona a integraÃ§Ã£o com bureaus externos?",
    "Que tipo de comprovantes sÃ£o aceitos?",
    "Qual Ã© o processo de aprovaÃ§Ã£o de emprÃ©stimos?",
    "Como Ã© calculado o score de crÃ©dito?"
]

async def create_test_dataset():
    """Cria um dataset de teste executando as perguntas no sistema RAG"""
    print("ğŸš€ Criando dataset de teste...")
    
    # Inicializar serviÃ§os
    await database_service.create_tables()
    rag_service = RAGService()
    
    interactions_created = []
    
    for i, question in enumerate(TEST_QUESTIONS, 1):
        print(f"ğŸ“ Processando pergunta {i}/{len(TEST_QUESTIONS)}: {question[:50]}...")
        
        try:
            # Executar pergunta no sistema RAG
            response = await rag_service.ask_question(
                question=question,
                max_documents=3,
                save_interaction=True
            )
            
            if "interaction_id" in response:
                interactions_created.append(response["interaction_id"])
                print(f"âœ… InteraÃ§Ã£o criada: {response['interaction_id']}")
            else:
                print(f"âš ï¸  Sem ID de interaÃ§Ã£o para: {question[:30]}...")
                
        except Exception as e:
            print(f"âŒ Erro ao processar '{question[:30]}...': {str(e)}")
    
    print(f"\nâœ¨ Dataset criado com {len(interactions_created)} interaÃ§Ãµes!")
    return interactions_created

async def run_ragas_evaluation(interaction_ids):
    """Executa avaliaÃ§Ã£o RAGAS no dataset criado"""
    print("\nğŸ” Executando avaliaÃ§Ã£o RAGAS...")
    
    try:
        results = await ragas_service.evaluate_interactions(
            interaction_ids=interaction_ids[:5]  # Avaliar apenas as primeiras 5 para teste
        )
        
        if "error" in results:
            print(f"âŒ Erro na avaliaÃ§Ã£o: {results['error']}")
            return
        
        print("ğŸ“Š Resultados da AvaliaÃ§Ã£o RAGAS:")
        print(f"   Total de interaÃ§Ãµes avaliadas: {results['total_interactions']}")
        print("\nğŸ“ˆ Scores MÃ©dios:")
        
        avg_scores = results['average_scores']
        for metric, score in avg_scores.items():
            print(f"   {metric}: {score:.3f}")
        
        # Mostrar alguns exemplos individuais
        if results['individual_scores']:
            print("\nğŸ” Exemplos de Scores Individuais:")
            for i, score in enumerate(results['individual_scores'][:3]):
                print(f"   Pergunta {i+1}: {score['question']}")
                print(f"   - Faithfulness: {score.get('faithfulness', 'N/A')}")
                print(f"   - Answer Relevancy: {score.get('answer_relevancy', 'N/A')}")
                print(f"   - Context Precision: {score.get('context_precision', 'N/A')}")
                print()
        
    except Exception as e:
        print(f"âŒ Erro durante avaliaÃ§Ã£o: {str(e)}")

async def get_quality_report():
    """Gera e exibe relatÃ³rio de qualidade"""
    print("\nğŸ“‹ Gerando RelatÃ³rio de Qualidade...")
    
    try:
        report = await ragas_service.get_quality_report(days=1)  # Ãšltimo dia
        
        if "error" in report:
            print(f"âš ï¸  {report['error']}")
            return
        
        print(f"ğŸ“Š RelatÃ³rio: {report['period']}")
        print(f"   Total de interaÃ§Ãµes: {report['total_interactions']}")
        
        print("\nğŸ“ˆ MÃ©tricas por Categoria:")
        for metric, stats in report['metrics'].items():
            if stats['count'] > 0:
                print(f"   {metric}:")
                print(f"   - MÃ©dia: {stats['mean']:.3f}")
                print(f"   - Min/Max: {stats['min']:.3f} / {stats['max']:.3f}")
                print(f"   - Amostras: {stats['count']}")
        
        overall = report['overall_quality']['average_score']
        print(f"\nğŸ¯ Score Geral de Qualidade: {overall:.3f}")
        
    except Exception as e:
        print(f"âŒ Erro ao gerar relatÃ³rio: {str(e)}")

async def main():
    """FunÃ§Ã£o principal do teste"""
    print("ğŸ§ª Teste do Sistema RAGAS")
    print("=" * 50)
    
    try:
        # Etapa 1: Criar dataset de teste
        interaction_ids = await create_test_dataset()
        
        if not interaction_ids:
            print("âŒ Nenhuma interaÃ§Ã£o foi criada. Verifique o sistema RAG.")
            return
        
        # Etapa 2: Executar avaliaÃ§Ã£o RAGAS
        await run_ragas_evaluation(interaction_ids)
        
        # Etapa 3: Gerar relatÃ³rio de qualidade
        await get_quality_report()
        
        print("\nâœ… Teste concluÃ­do com sucesso!")
        print("\nğŸ’¡ PrÃ³ximos passos:")
        print("   1. Execute o servidor: python main.py")
        print("   2. Acesse os endpoints de avaliaÃ§Ã£o em /api/v1/evaluation/")
        print("   3. Use /docs para explorar a API")
        
    except Exception as e:
        print(f"âŒ Erro durante o teste: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())