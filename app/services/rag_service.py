# RAG SERVICE - Orquestrador Principal do Sistema RAG
# Este √© o "maestro" que coordena todo o pipeline RAG:
# 1. Busca documentos relevantes (VectorService)
# 2. Gera resposta com LLM (LLMService)
# 3. Salva intera√ß√£o no banco (DatabaseService)
# 4. Monitora com Phoenix (PhoenixService)
# 5. Disponibiliza dados para RAGAS avaliar depois

from typing import List, Dict, Any, Optional
import time  # Medir tempo de resposta
import uuid  # Gerar IDs √∫nicos para intera√ß√µes
from app.services.vector_service import VectorService  # Busca vetorial de documentos
from app.services.llm_service import LLMService  # Gera√ß√£o de respostas com GPT
from app.models.document import DocumentResponse  # Modelo de documento
from app.models.rag_interaction import RAGInteractionDB, RAGInteractionCreate  # Modelos de intera√ß√£o
from app.services.database_service import AsyncSessionLocal  # Conex√£o com banco de dados
from app.services.phoenix_service import phoenix_service  # Observabilidade
import logging

class RAGService:
    """
    SERVI√áO PRINCIPAL DO SISTEMA RAG
    
    Este √© o "maestro" que orquestra todo o pipeline RAG:
    
    PIPELINE COMPLETO:
    Pergunta do Usu√°rio
           ‚Üì
    1. VectorService: Busca documentos similares
           ‚Üì
    2. LLMService: Gera resposta baseada nos documentos
           ‚Üì
    3. DatabaseService: Salva intera√ß√£o completa
           ‚Üì
    4. PhoenixService: Monitora para observabilidade
           ‚Üì
    5. RAGAS: Pode avaliar qualidade depois
           ‚Üì
    Resposta + Fontes para o Usu√°rio
    
    INTEGRA√á√ïES:
    - LangChain: Via LLMService para gera√ß√£o de resposta
    - Phoenix: Monitoramento autom√°tico de todo o pipeline
    - RAGAS: Usa dados salvos para avaliar qualidade
    """
    
    def __init__(self):
        # Inicializa os servi√ßos que comp√µem o pipeline RAG
        self.vector_service = VectorService()  # Busca vetorial (ChromaDB + embeddings)
        self.llm_service = LLMService()        # Gera√ß√£o de resposta (LangChain + OpenAI)
    
    async def ask_question(
        self, 
        question: str, 
        max_documents: int = 5,
        category_filter: Optional[str] = None,
        save_interaction: bool = True
    ) -> Dict[str, Any]:
        """
        FUN√á√ÉO PRINCIPAL: Pipeline RAG Completo
        
        FLUXO PASSO A PASSO:
        1. üîç BUSCA: Vector search encontra documentos similares
        2. ü§ñ GERA√á√ÉO: LLM cria resposta baseada nos documentos
        3. üíæ PERSIST√äNCIA: Salva intera√ß√£o no banco (para RAGAS avaliar)
        4. üî• OBSERVABILIDADE: Phoenix monitora toda a opera√ß√£o
        5. üì§ RESPOSTA: Retorna resposta + fontes para o usu√°rio
        
        INTEGRA√á√ïES ATIVAS:
        - LangChain: Gera√ß√£o de resposta estruturada
        - Phoenix: Monitoramento autom√°tico via instrumenta√ß√£o
        - RAGAS: Dados salvos ficam dispon√≠veis para avalia√ß√£o
        
        Args:
            question: Pergunta do usu√°rio
            max_documents: M√°ximo de documentos para usar como contexto (default=5)
            category_filter: Filtro opcional por categoria de documento
            save_interaction: Se deve salvar a intera√ß√£o no banco (default=True para RAGAS)
            
        Returns:
            Dict com resposta gerada, fontes citadas, metadados e tempo de resposta
        """
        # ‚è±Ô∏è CRONOMETRO: Medir tempo total de resposta
        start_time = time.time()
        
        # üîç ETAPA 1: BUSCAR DOCUMENTOS RELEVANTES
        # Vector search: converte pergunta em embedding e busca documentos similares
        print(f"üîç Buscando documentos relevantes para: '{question[:50]}...'")
        relevant_docs = await self.vector_service.search_documents(
            query=question,                # Pergunta vira embedding
            limit=max_documents,           # Top-K documentos mais similares
            category_filter=category_filter # Filtro opcional por categoria
        )
        print(f"üìù Encontrados {len(relevant_docs)} documentos relevantes")
        
        # ‚ö†Ô∏è CASO ESPECIAL: N√ÉO ENCONTROU DOCUMENTOS RELEVANTES
        if not relevant_docs:
            print("‚ö†Ô∏è Nenhum documento relevante encontrado")
            response = {
                "answer": "Desculpe, n√£o encontrei documentos relevantes para responder sua pergunta.",
                "sources": [],
                "context_used": 0,
                "question": question,
                "has_context": False
            }
            
            # Salvar intera√ß√£o mesmo sem contexto (importante para m√©tricas)
            if save_interaction:
                await self._save_interaction(
                    question=question,
                    answer=response["answer"],
                    contexts=[],             # Lista vazia
                    sources=[],              # Sem fontes
                    response_time=time.time() - start_time
                )
            
            return response
        
        # üîÑ ETAPA 2: CONVERTER DOCUMENTOS PARA FORMATO DO LLM
        # Prepara dados em dois formatos:
        # - context_documents: formato rico para LLM (com metadados)
        # - contexts_for_db: apenas texto para salvar no banco (RAGAS usar√°)
        context_documents = []  # Para LLMService
        contexts_for_db = []    # Para banco de dados
        
        print(f"üîÑ Preparando {len(relevant_docs)} documentos para o LLM:")
        for doc in relevant_docs:
            # Formato rico para o LLM
            context_documents.append({
                "title": doc.title,
                "category": doc.category,
                "content": doc.content,
                "similarity_score": doc.similarity_score,  # Qu√£o similar √† pergunta (0-1)
                "metadata": doc.metadata
            })
            # Apenas o conte√∫do para salvar no banco (RAGAS precisa s√≥ do texto)
            contexts_for_db.append(doc.content)
            print(f"   ‚Ä¢ {doc.title} (similaridade: {doc.similarity_score:.3f})")
        
        # ü§ñ ETAPA 3: GERAR RESPOSTA COM LLM (LANGCHAIN + OPENAI)
        # Aqui √© onde a "m√°gica" acontece: GPT l√™ os documentos e gera a resposta
        print(f"ü§ñ Gerando resposta com LLM...")
        llm_response = await self.llm_service.generate_answer(
            question=question,              # Pergunta original
            context_documents=context_documents  # Documentos como contexto
        )
        print(f"‚úÖ Resposta gerada: '{llm_response.get('answer', '')[:100]}...'")
        
        # üéÜ ETAPA 4: ENRIQUECER RESPOSTA COM INFORMA√á√ïES ADICIONAIS
        # Prepara informa√ß√µes extras para mostrar ao usu√°rio
        search_results = [
            {
                "title": doc.title,
                "category": doc.category,
                "similarity_score": doc.similarity_score,
                # Preview do conte√∫do (primeiros 200 caracteres)
                "content_preview": doc.content[:200] + "..." if len(doc.content) > 200 else doc.content
            }
            for doc in relevant_docs
        ]
        
        # Combina resposta do LLM com informa√ß√µes extras
        response = {
            **llm_response,                    # Resposta + fontes do LLM
            "has_context": True,              # Flag indicando que tem contexto
            "search_results": search_results  # Detalhes dos documentos encontrados
        }
        
        # üíæ ETAPA 5: SALVAR INTERA√á√ÉO NO BANCO DE DADOS
        # Essencial para RAGAS avaliar qualidade depois
        response_time = time.time() - start_time
        print(f"‚è±Ô∏è Tempo total de resposta: {response_time:.2f}s")
        
        if save_interaction:
            print(f"üíæ Salvando intera√ß√£o no banco de dados...")
            interaction_id = await self._save_interaction(
                question=question,                      # Pergunta original
                answer=response["answer"],              # Resposta gerada
                contexts=contexts_for_db,               # Textos usados como contexto
                sources=response.get("sources", []),    # Fontes citadas
                response_time=response_time             # Performance
            )
            response["interaction_id"] = interaction_id
            print(f"‚úÖ Intera√ß√£o salva com ID: {interaction_id}")
            
            # üî• ETAPA 6: LOG NO PHOENIX PARA OBSERVABILIDADE
            # Phoenix monitora automaticamente via instrumenta√ß√£o, mas podemos adicionar logs extras
            if phoenix_service.is_enabled:
                print(f"üî• Enviando dados para Phoenix dashboard...")
                phoenix_service.log_rag_interaction(
                    question=question,
                    answer=response["answer"],
                    contexts=contexts_for_db,
                    sources=response.get("sources", []),
                    response_time=response_time,
                    metadata={
                        "interaction_id": interaction_id,        # ID √∫nico da intera√ß√£o
                        "max_documents": max_documents,          # Par√¢metros da busca
                        "category_filter": category_filter,     # Filtros aplicados
                        "has_context": response.get("has_context", False),  # Se encontrou contexto
                        "context_used": response.get("context_used", 0)     # Quantos docs usou
                    }
                )
                print(f"‚úÖ Dados enviados para Phoenix - dashboard em: {phoenix_service.get_phoenix_url()}")
        
        print(f"üéâ Pipeline RAG conclu√≠do com sucesso!")
        return response
    
    async def _save_interaction(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        sources: List[Dict],
        response_time: float
    ) -> str:
        """Salva a intera√ß√£o RAG no banco de dados
        
        IMPORTANTE: Esses dados s√£o essenciais para:
        - RAGAS avaliar qualidade das respostas
        - Acompanhar performance ao longo do tempo
        - Gerar relat√≥rios de uso
        - Identificar padr√µes e problemas
        
        DADOS SALVOS:
        - question: Pergunta original do usu√°rio
        - answer: Resposta gerada pelo LLM
        - contexts: Textos dos documentos usados como contexto
        - sources: Metadados das fontes citadas
        - response_time: Tempo que demorou para responder
        
        Returns:
            ID √∫nico da intera√ß√£o salva
        """
        # Gerar ID √∫nico para a intera√ß√£o
        interaction_id = str(uuid.uuid4())
        
        # Salvar no banco de dados
        async with AsyncSessionLocal() as session:
            interaction = RAGInteractionDB(
                id=interaction_id,
                question=question,          # RAGAS usar√° isso para avaliar
                answer=answer,              # RAGAS avaliar√° se √© boa resposta
                contexts=contexts,          # RAGAS avaliar√° se contexto √© relevante
                sources=sources,            # Metadados das fontes
                response_time=response_time # M√©trica de performance
            )
            
            session.add(interaction)
            await session.commit()
            
        return interaction_id