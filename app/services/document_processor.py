# DOCUMENT PROCESSOR - Servi√ßo de Processamento e Limpeza de Documentos
# Este servi√ßo prepara documentos para serem usados no sistema RAG:
# 1. L√™ arquivos de texto
# 2. Limpa conte√∫do (remove ru√≠do, dados fict√≠cios)
# 3. Divide em chunks usando LangChain
# 4. Calcula perplexidade para detectar texto de baixa qualidade

import os
import re
from typing import List, Set
import spacy  # Processamento de linguagem natural (PNL)
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Divisor de texto do LangChain
from langchain.schema import Document as LangChainDocument  # Tipo de documento do LangChain
from app.models.document import Document  # Modelo de documento do projeto
import numpy as np  # C√°lculos num√©ricos
from transformers import AutoTokenizer, AutoModelForCausalLM  # Modelo HuggingFace para perplexidade
from pathlib import Path
import torch  # Framework de deep learning


class DocumentProcessor:
    """
    Processador de documentos que prepara arquivos para o sistema RAG
    
    FUNCIONALIDADES:
    1. Carrega documentos de arquivos .txt
    2. Extrai metadados (t√≠tulo, categoria)
    3. Limpa conte√∫do usando perplexidade (remove texto de baixa qualidade)
    4. Divide documentos em chunks menores usando LangChain
    
    TECNOLOGIAS USADAS:
    - LangChain: Divis√£o inteligente de texto
    - spaCy: Processamento de linguagem natural
    - HuggingFace: Modelo para calcular perplexidade
    - PyTorch: Backend para o modelo
    """
    
    def __init__(self):
        # CONFIGURAR DIVISOR DE TEXTO (LANGCHAIN)
        # Divide documentos grandes em peda√ßos menores para melhor retrieval
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,                           # Tamanho m√°ximo de cada peda√ßo
            chunk_overlap=50,                         # Sobreposi√ß√£o entre peda√ßos (evita perder contexto)
            length_function=len,                      # Como medir tamanho (caracteres)
            separators=["\n\n", "\n", " ", ""]        # Onde preferir quebrar (par√°grafo > linha > espa√ßo)
        )
        
        # CARREGAR MODELO SPACY PARA PROCESSAMENTO DE TEXTO
        try:
            self.nlp = spacy.load("pt_core_news_lg")  # Modelo grande (melhor qualidade)
        except OSError:
            try:
                self.nlp = spacy.load("pt_core_news_sm")  # Modelo pequeno (fallback)
            except OSError:
                print("‚ö†Ô∏è  Instale modelo spaCy: python -m spacy download pt_core_news_lg")
                self.nlp = None
        
        # CARREGAR MODELO PARA C√ÅLCULO DE PERPLEXIDADE
        # Perplexidade mede "surpresa" - texto estranho tem alta perplexidade
        try:
            self.model_name = "pierreguillou/gpt2-small-portuguese"  # Modelo leve em portugu√™s
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)  # Converte texto em n√∫meros
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)  # Modelo neural
            self.PERPLEXITY_THRESHOLD = 600  # Frases com perplexidade > 600 s√£o suspeitas
        except:
            print("‚ö†Ô∏è  Erro ao carregar modelo HuggingFace para perplexidade")
            self.tokenizer = None
            self.model = None
        

    def load_documents_from_directory(self, directory_path: str) -> List[Document]:
        """Carrega e processa todos os documentos .txt de um diret√≥rio
        
        PROCESSO COMPLETO:
        1. Lista todos os arquivos .txt no diret√≥rio
        2. L√™ cada arquivo
        3. Extrai metadados (t√≠tulo, categoria) do conte√∫do
        4. Limpa o conte√∫do (remove ru√≠do)
        5. Cria objeto Document padronizado
        
        Args:
            directory_path: Caminho para o diret√≥rio com arquivos .txt
            
        Returns:
            Lista de documentos processados e limpos
        """
        documents = []
        
        # Verifica se diret√≥rio existe
        if not os.path.exists(directory_path):
            raise FileNotFoundError(f"Diret√≥rio n√£o encontrado: {directory_path}")
        
        # Processa cada arquivo .txt do diret√≥rio
        for filename in os.listdir(directory_path):
            if filename.endswith('.txt'):
                file_path = os.path.join(directory_path, filename)
                
                try:
                    # PASSO 1: LER ARQUIVO
                    with open(file_path, 'r', encoding='utf-8') as file:
                        content = file.read().strip()
                    
                    # PASSO 2: EXTRAIR METADADOS
                    title, category = self._extract_metadata_from_content(content)
                    
                    # PASSO 3: LIMPAR CONTE√öDO (remove ru√≠do, dados fict√≠cios)
                    clean_content = self._clean_content(content)
                    
                    # PASSO 4: VERIFICAR SE SOBROU CONTE√öDO DEPOIS DA LIMPEZA
                    if not clean_content.strip():
                        print(f"‚ö†Ô∏è  Arquivo vazio ap√≥s limpeza: {filename}")
                        continue
                    
                    # PASSO 5: CRIAR OBJETO DOCUMENT PADRONIZADO
                    document = Document(
                        title=title or filename.replace('.txt', ''),  # T√≠tulo ou nome do arquivo
                        category=category or "sem_categoria",         # Categoria ou padr√£o
                        content=clean_content,                        # Conte√∫do limpo
                        metadata={
                            "source_file": filename,
                            "file_path": file_path
                        }
                    )
                    
                    documents.append(document)
                    print(f"‚úÖ Documento processado: {filename} (t√≠tulo: {title})")
                
                except Exception as e:
                    print(f"‚ùå Erro ao processar {filename}: {e}")
                    continue
        
        print(f"üìö Total de documentos carregados: {len(documents)}")
        return documents
    
   

    def _extract_metadata_from_content(self, content: str) -> tuple:
        """Extrai metadados (t√≠tulo e categoria) do conte√∫do do arquivo
        
        Procura por linhas que come√ßam com:
        - "T√≠tulo: ..."
        - "Categoria: ..."
        
        Args:
            content: Conte√∫do completo do arquivo
            
        Returns:
            Tuple (titulo, categoria) ou ("", "") se n√£o encontrar
        """
        lines = content.split('\n')
        title = ""
        category = ""
        
        # Procura por linhas de metadados
        for line in lines:
            if line.startswith('T√≠tulo:'):
                title = line.replace('T√≠tulo:', '').strip()
            elif line.startswith('Categoria:'):
                category = line.replace('Categoria:', '').strip()
        
        return title, category
    
    def _clean_content(self, content: str) -> str:
        """Limpa o conte√∫do removendo frases de baixa qualidade usando perplexidade
        
        PROCESSO DE LIMPEZA:
        1. Divide texto em senten√ßas usando spaCy
        2. Calcula perplexidade de cada senten√ßa
        3. Remove senten√ßas com perplexidade muito alta (provavelmente ru√≠do)
        4. Mant√©m apenas senten√ßas de boa qualidade
        
        PERPLEXIDADE: mede qu√£o "surpresa" √© uma frase para o modelo
        - Baixa perplexidade = texto normal, bem estruturado
        - Alta perplexidade = texto estranho, possivelmente ru√≠do ou dados fict√≠cios
        
        Args:
            content: Texto bruto do documento
            
        Returns:
            Texto limpo, apenas com senten√ßas de boa qualidade
        """
        if not self.nlp or not self.model:
            print("‚ö†Ô∏è  Modelos n√£o carregados, retornando conte√∫do sem limpeza")
            return content

        # ETAPA 1: DIVIDIR TEXTO EM SENTEN√áAS USANDO SPACY
        doc = self.nlp(content)
        good_phrases = []      # Senten√ßas de boa qualidade
        suspect_phrases = []   # Senten√ßas suspeitas (alta perplexidade)

        for sent in doc.sents:
            sentence_text = sent.text.strip()  # Remove espa√ßos extras
            
            # Ignora senten√ßas muito curtas (provavelmente n√£o s√£o informativas)
            if len(sentence_text.split()) < 3:
                continue

            # ETAPA 2: CALCULAR PERPLEXIDADE
            ppl = self._calculate_perplexity(sentence_text, self.model, self.tokenizer)

            # LISTA DE EXCE√á√ïES (falsos positivos conhecidos)
            # Essas frases t√™m alta perplexidade mas s√£o leg√≠timas
            EXCEPTIONS = [
                "O colaborador deve enviar documentos via plataforma digital.",
                "Clientes negativados devem quitar d√≠vidas anteriores antes de nova an√°lise."
            ]
            
            # LISTA DE FRASES CONHECIDAMENTE RU√çDO
            # For√ßa remo√ß√£o independente da perplexidade
            KNOWN_NOISE = [
                "Dados fict√≠cios devem ser ignorados pelo modelo.",
                "Este par√°grafo √© um exemplo de ru√≠do textual proposital."
            ]
            
            # ETAPA 3: DECIDIR SE MANTER OU REMOVER A SENTEN√áA
            if (ppl > self.PERPLEXITY_THRESHOLD and sentence_text not in EXCEPTIONS) or sentence_text in KNOWN_NOISE:
                suspect_phrases.append({"texto": sentence_text, "perplexidade": ppl})
                print(f"‚ùå Removida (perplexidade {ppl:.1f}): {sentence_text[:50]}...")
            else:
                good_phrases.append({"texto": sentence_text, "perplexidade": ppl})
        
        print(f"‚úÖ Limpeza conclu√≠da: {len(good_phrases)} frases mantidas, {len(suspect_phrases)} removidas")
        
        # ETAPA 4: RETORNAR TEXTO LIMPO
        return ' '.join([phrase["texto"] for phrase in good_phrases])
    

        
    def chunk_document(self, document: Document) -> List[Document]:
        """Divide um documento grande em peda√ßos menores usando LangChain
        
        POR QUE DIVIDIR DOCUMENTOS?
        - Embeddings funcionam melhor com textos menores
        - Busca vetorial fica mais precisa
        - LLMs t√™m limite de contexto
        - Usu√°rio v√™ cita√ß√µes mais espec√≠ficas
        
        COMO LANGCHAIN DIVIDE:
        1. Tenta quebrar em par√°grafos (\n\n)
        2. Se n√£o conseguir, quebra por linha (\n)
        3. Se n√£o conseguir, quebra por espa√ßo
        4. Mant√©m sobreposi√ß√£o para n√£o perder contexto
        
        Args:
            document: Documento original grande
            
        Returns:
            Lista de documentos menores (chunks)
        """
        # Usa o text_splitter do LangChain para dividir inteligentemente
        chunks = self.text_splitter.split_text(document.content)
        
        chunked_documents = []
        for i, chunk in enumerate(chunks):
            # Cria um documento separado para cada chunk
            chunked_doc = Document(
                title=f"{document.title} - Chunk {i+1}",    # T√≠tulo indica que √© um peda√ßo
                category=document.category,                   # Mant√©m categoria original
                content=chunk,                               # Conte√∫do do peda√ßo
                metadata={
                    **document.metadata,                     # Metadados originais
                    "chunk_index": i,                       # √çndice do peda√ßo
                    "total_chunks": len(chunks)             # Total de peda√ßos
                }
            )
            chunked_documents.append(chunked_doc)
        
        print(f"üìù Documento '{document.title}' dividido em {len(chunks)} chunks")
        return chunked_documents
    
    def _calculate_perplexity(self, sentence, model, tokenizer):
        """Calcula a perplexidade de uma senten√ßa usando modelo de linguagem
        
        PERPLEXIDADE EXPLICADA:
        - √â uma medida de "surpresa" do modelo ao ver o texto
        - Baixa perplexidade = texto comum, bem escrito
        - Alta perplexidade = texto estranho, possivelmente ru√≠do
        
        COMO FUNCIONA:
        1. Modelo tenta "prever" cada palavra da senten√ßa
        2. Calcula qu√£o "errado" ele estava (loss)
        3. Transforma o erro em medida de surpresa (perplexidade)
        
        Args:
            sentence: Texto para calcular perplexidade
            model: Modelo de linguagem (GPT-2 portugu√™s)
            tokenizer: Conversor texto->n√∫meros
            
        Returns:
            Valor de perplexidade (quanto maior, mais "estranho" o texto)
        """
        if not sentence.strip():
            return 0.0
            
        # ETAPA 1: CONVERTER TEXTO EM N√öMEROS
        # Modelos neurais trabalham com n√∫meros, n√£o texto
        inputs = tokenizer(sentence, return_tensors="pt")
        
        # ETAPA 2: VERIFICAR SE SENTEN√áA N√ÉO √â MUITO LONGA
        # Modelos t√™m limite de tokens que conseguem processar
        if inputs.input_ids.size(1) > model.config.n_positions:
            return float('inf')  # Perplexidade infinita = "muito estranho"
            
        # ETAPA 3: CALCULAR PERPLEXIDADE
        with torch.no_grad():  # N√£o vamos treinar, s√≥ calcular (mais r√°pido)
            # Modelo tenta prever cada palavra e calcula o erro
            outputs = model(**inputs, labels=inputs["input_ids"])
            
            # Loss = quanto o modelo "errou" nas previs√µes
            loss = outputs.loss
            
        # ETAPA 4: CONVERTER LOSS EM PERPLEXIDADE
        # F√≥rmula matem√°tica padr√£o: perplexidade = e^loss
        ppl = torch.exp(loss)
        
        return ppl.item()  # Converte tensor PyTorch em n√∫mero Python